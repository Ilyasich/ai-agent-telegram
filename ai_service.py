import os
import logging
from openai import OpenAI, AsyncOpenAI
import config

# Initialize Groq client using OpenAI-compatible API
# Groq provides fast, free inference with generous rate limits
client = AsyncOpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key=config.GROQ_API_KEY
)

# Model selection - Groq's fast models
# Options: "llama3-8b-8192" (faster) or "mixtral-8x7b-32768" (more capable)
GROQ_MODEL = "llama-3.3-70b-versatile"

SYSTEM_PROMPT = """
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –ª–æ–≥ —á–∞—Ç–∞ —Å –∫–æ–º–∞–Ω–¥–Ω–æ–π –≤—Å—Ç—Ä–µ—á–∏. –ò–≥–Ω–æ—Ä–∏—Ä—É–π —Å–≤–µ—Ç—Å–∫–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã –∏ —Å–ø–∞–º.
–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –≤—ã–≤–æ–¥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏:
üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –¶–µ–ª–∏
üí° –ö–ª—é—á–µ–≤—ã–µ –ò–¥–µ–∏
‚úÖ –ó–∞–¥–∞—á–∏ (–ö—Ç–æ - –ß—Ç–æ)
ü§ù –ü—Ä–∏–Ω—è—Ç—ã–µ –†–µ—à–µ–Ω–∏—è

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
"""

MAX_CHARS = 15000

def chunk_text(text, max_chars=MAX_CHARS):
    """Splits text into chunks of max_chars, respecting line breaks."""
    chunks = []
    current_chunk = ""
    
    for line in text.split('\n'):
        if len(current_chunk) + len(line) + 1 > max_chars:
            chunks.append(current_chunk)
            current_chunk = line + "\n"
        else:
            current_chunk += line + "\n"
            
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

async def summarize_chunk(text):
    """Summarize a single chunk of text using Groq."""
    try:
        response = await client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": f"Chat Log:\n\n{text}"}
            ],
            temperature=0.5,
            max_tokens=2000,  # Limit response length
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"Error in summarize_chunk: {e}")
        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ AI —Å–µ—Ä–≤–∏—Å–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–¥–∫—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

async def summarize_chat(chat_text):
    """Generate a summary of chat history, with chunking support."""
    if not chat_text:
        return "–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–æ–¥–∫–∏."

    try:
        if len(chat_text) <= MAX_CHARS:
            return await summarize_chunk(chat_text)
        
        # Chunking logic for long conversations
        chunks = chunk_text(chat_text)
        chunk_summaries = []
        
        for chunk in chunks:
            summary = await summarize_chunk(chunk)
            # Check if summary is an error message
            if summary.startswith("‚ö†Ô∏è"):
                return summary  # Return error immediately
            chunk_summaries.append(summary)
            
        # Summarize the summaries if there are multiple chunks
        combined_summary_text = "\n\n".join(chunk_summaries)
        
        final_response = await client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": "You are consolidating multiple meeting summaries into one cohesive report. Keep the same structure: Goals, Ideas, Action Items, Decisions."},
                {"role": "user", "content": f"Summaries to consolidate:\n\n{combined_summary_text}"}
            ],
            temperature=0.5,
            max_tokens=2000,
        )
        return final_response.choices[0].message.content
        
    except Exception as e:
        logging.error(f"Error in summarize_chat: {e}")
        return "‚ö†Ô∏è AI —Å–µ—Ä–≤–∏—Å—ã —Å–µ–π—á–∞—Å –∑–∞–Ω—è—Ç—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É."

# Intent Detection System Prompt
INTENT_SYSTEM_PROMPT = """
–¢—ã - –°—Ç—Ä–æ–≥–∏–π –ê–Ω–∞–ª–∏—Ç–∏–∫ –î–∞–Ω–Ω—ã—Ö.

–¢–í–û–Ø –ó–ê–î–ê–ß–ê:
–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –û–î–ù–û –∏–∑ –¥–≤—É—Ö –¥–µ–π—Å—Ç–≤–∏–π: "summary" –∏–ª–∏ "search".

–ü–†–ê–í–ò–õ–ê:
1. –ù–ò–ö–ê–ö–ò–• —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤. –ù–ò–ö–ê–ö–ò–• "chat" –∏–Ω—Ç–µ–Ω—Ç–æ–≤.
2. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–¥–æ—Ä–æ–≤–∞–µ—Ç—Å—è ("–ü—Ä–∏–≤–µ—Ç", "Hello") -> action="search", keywords="" (–ø–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏).
3. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç ("–°–ø–∞—Å–∏–±–æ") -> action="search", keywords="" (–ø–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏).
4. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç "–ß—Ç–æ –Ω–æ–≤–æ–≥–æ?", "–ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏" -> action="search", keywords="".
5. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ç–µ–º—É -> action="search", keywords="—Ç–µ–º–∞".
6. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç —Å–≤–æ–¥–∫—É –∑–∞ –≤—Ä–µ–º—è -> action="summary".

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):

1. **summary**:
   - {"action": "summary", "timeframe": "1d"} (1h, 1d, 1w, 1m)

2. **search**:
   - {"action": "search", "keywords": "—Å—Ç—Ä–æ–∫–∞ –ø–æ–∏—Å–∫–∞", "username": "–∏–º—è –∏–ª–∏ null"}
   - keywords="" (–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞) –æ–∑–Ω–∞—á–∞–µ—Ç "–ü–û–°–õ–ï–î–ù–ò–ï –ù–û–í–û–°–¢–ò".

–ü—Ä–∏–º–µ—Ä—ã:
User: "–ü—Ä–∏–≤–µ—Ç"
Response: {"action": "search", "keywords": "", "username": null}

User: "–ß—Ç–æ —Ç–∞–º –ø—Ä–æ –Ω–∞–ª–æ–≥–∏?"
Response: {"action": "search", "keywords": "–Ω–∞–ª–æ–≥–∏", "username": null}

User: "–î–∞–π —Å–≤–æ–¥–∫—É –∑–∞ –≤—á–µ—Ä–∞"
Response: {"action": "summary", "timeframe": "1d"}
"""

async def detect_intent(user_text: str) -> dict:
    """
    Detect user intent using Groq.
    
    Args:
        user_text: The user's message text
        
    Returns:
        dict: {"action": "summary", "timeframe": "1d"} or
              {"action": "chat", "reply": "response text"}
    """
    try:
        response = await client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": INTENT_SYSTEM_PROMPT},
                {"role": "user", "content": user_text}
            ],
            temperature=0.3,  # Lower temperature for more consistent JSON output
            max_tokens=500,
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # Try to parse JSON
        import json
        try:
            intent_data = json.loads(result_text)
            
            # Validate the response structure
            if "action" not in intent_data:
                raise ValueError("Missing 'action' field in response")
                
            if intent_data["action"] == "summary":
                # Ensure timeframe exists and is valid
                if "timeframe" not in intent_data:
                    intent_data["timeframe"] = "1h"  # Default
                elif intent_data["timeframe"] not in ["1h", "1d", "1w", "1m", "all"]:
                    intent_data["timeframe"] = "1h"  # Fallback to default
                    
            elif intent_data["action"] == "search":
                # Ensure keywords exists
                if "keywords" not in intent_data:
                    intent_data["keywords"] = "" # Empty means "latest"
                
                # Ensure username is present (can be null)
                if "username" not in intent_data:
                    intent_data["username"] = None
            
            # Force any unknown action to search latest
            else:
                intent_data = {"action": "search", "keywords": "", "username": None}
                
            return intent_data
            
        except json.JSONDecodeError as e:
            logging.error(f"Failed to parse JSON from LLM: {result_text}")
            # Fallback to search latest
            return {"action": "search", "keywords": "", "username": None}
            
    except Exception as e:
        logging.error(f"Error in detect_intent: {str(e)}")
        # Fallback response
        return {"action": "search", "keywords": "", "username": None}

async def answer_search_query(user_question: str, found_messages: list = None, context_text: str = None) -> str:
    """
    Generate a contextual answer based on search results OR direct context.
    
    Args:
        user_question: The original user's question
        found_messages: List of tuples (username, text, created_at) from database
        context_text: Direct text from a forwarded/replied message
        
    Returns:
        str: AI-generated answer
    """
    
    # Prepare the context for the AI
    data_context = ""
    
    if context_text:
        data_context = f"–ö–û–ù–¢–ï–ö–°–¢ (–∏–∑ –ø–µ—Ä–µ—Å–ª–∞–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è):\n{context_text}\n"
    elif found_messages:
        # Format messages for AI
        msgs = []
        for username, text, created_at in found_messages:
            msgs.append(f"[{created_at}] {username}: {text}")
        data_context = "–ù–ê–ô–î–ï–ù–ù–´–ï –°–û–û–ë–©–ï–ù–ò–Ø –ò–ó –ë–ê–ó–´:\n" + "\n".join(msgs)
    else:
        return "–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –í –±–∞–∑–µ –Ω–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."
    
    search_prompt = f"""
–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: "{user_question}"

{data_context}

–ò–ù–°–¢–†–£–ö–¶–ò–Ø:
–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
"""
    
    try:
        response = await client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": """
–¢—ã - –°—Ç—Ä–æ–≥–∏–π –ê–Ω–∞–ª–∏—Ç–∏–∫ –ù–æ–≤–æ—Å—Ç–µ–π.

–ü–†–ê–í–ò–õ–ê:
1. –ë—É–¥—å –∫—Ä–∞—Ç–æ–∫. –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏.
2. –¢–æ–Ω: –§–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π. –ë–µ–∑ —ç–º–æ—Ü–∏–π.
3. –§–æ—Ä–º–∞—Ç –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π: "üïí [–í—Ä–µ–º—è] - [–°—É—Ç—å —Å–æ–±—ã—Ç–∏—è]".
4. –ï—Å–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –ø–µ—Ä–µ—Å–ª–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (–ö–æ–Ω—Ç–µ–∫—Å—Ç), –ø—Ä–æ—Å—Ç–æ –¥–∞–π –µ–≥–æ –∫—Ä–∞—Ç–∫—É—é —Å—É—Ç—å/–æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å.
5. –ù–ò–ö–û–ì–î–ê –Ω–µ –≥–æ–≤–æ—Ä–∏ "—è –Ω–µ –º–æ–≥—É", "—è –±–æ—Ç", "–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞".
6. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ, –ø—Ä–æ—Å—Ç–æ —Å–∫–∞–∂–∏ —Ç–æ, —á—Ç–æ –µ—Å—Ç—å.

–û—Ç–≤–µ—á–∞–π –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï.
"""},
                {"role": "user", "content": search_prompt}
            ],
            temperature=0.3,
            max_tokens=1000,
        )
        
        answer = response.choices[0].message.content
        return answer
        
    except Exception as e:
        logging.error(f"Error in answer_search_query: {e}")
        return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
