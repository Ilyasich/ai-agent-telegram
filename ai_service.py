import os
import logging
from openai import OpenAI, AsyncOpenAI
import config

# Initialize Groq client using OpenAI-compatible API
# Groq provides fast, free inference with generous rate limits
client = AsyncOpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key=config.GROQ_API_KEY
)

# Model selection - Groq's fast models
# Options: "llama3-8b-8192" (faster) or "mixtral-8x7b-32768" (more capable)
GROQ_MODEL = "llama-3.3-70b-versatile"

SYSTEM_PROMPT = """
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –ª–æ–≥ —á–∞—Ç–∞ —Å –∫–æ–º–∞–Ω–¥–Ω–æ–π –≤—Å—Ç—Ä–µ—á–∏. –ò–≥–Ω–æ—Ä–∏—Ä—É–π —Å–≤–µ—Ç—Å–∫–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã –∏ —Å–ø–∞–º.
–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –≤—ã–≤–æ–¥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏:
üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –¶–µ–ª–∏
üí° –ö–ª—é—á–µ–≤—ã–µ –ò–¥–µ–∏
‚úÖ –ó–∞–¥–∞—á–∏ (–ö—Ç–æ - –ß—Ç–æ)
ü§ù –ü—Ä–∏–Ω—è—Ç—ã–µ –†–µ—à–µ–Ω–∏—è

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
"""

MAX_CHARS = 15000

def chunk_text(text, max_chars=MAX_CHARS):
    """Splits text into chunks of max_chars, respecting line breaks."""
    chunks = []
    current_chunk = ""
    
    for line in text.split('\n'):
        if len(current_chunk) + len(line) + 1 > max_chars:
            chunks.append(current_chunk)
            current_chunk = line + "\n"
        else:
            current_chunk += line + "\n"
            
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

async def summarize_chunk(text):
    """Summarize a single chunk of text using Groq."""
    try:
        response = await client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": f"Chat Log:\n\n{text}"}
            ],
            temperature=0.5,
            max_tokens=2000,  # Limit response length
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"Error in summarize_chunk: {e}")
        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ AI —Å–µ—Ä–≤–∏—Å–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–¥–∫—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

async def summarize_chat(chat_text):
    """Generate a summary of chat history, with chunking support."""
    if not chat_text:
        return "–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–æ–¥–∫–∏."

    try:
        if len(chat_text) <= MAX_CHARS:
            return await summarize_chunk(chat_text)
        
        # Chunking logic for long conversations
        chunks = chunk_text(chat_text)
        chunk_summaries = []
        
        for chunk in chunks:
            summary = await summarize_chunk(chunk)
            # Check if summary is an error message
            if summary.startswith("‚ö†Ô∏è"):
                return summary  # Return error immediately
            chunk_summaries.append(summary)
            
        # Summarize the summaries if there are multiple chunks
        combined_summary_text = "\n\n".join(chunk_summaries)
        
        final_response = await client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": "You are consolidating multiple meeting summaries into one cohesive report. Keep the same structure: Goals, Ideas, Action Items, Decisions."},
                {"role": "user", "content": f"Summaries to consolidate:\n\n{combined_summary_text}"}
            ],
            temperature=0.5,
            max_tokens=2000,
        )
        return final_response.choices[0].message.content
        
    except Exception as e:
        logging.error(f"Error in summarize_chat: {e}")
        return "‚ö†Ô∏è AI —Å–µ—Ä–≤–∏—Å—ã —Å–µ–π—á–∞—Å –∑–∞–Ω—è—Ç—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É."

# Intent Detection System Prompt
INTENT_SYSTEM_PROMPT = """
–¢—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–º–µ—Ä–µ–Ω–∏–π –¥–ª—è Telegram –±–æ—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–∫–∏ –≤—Å—Ç—Ä–µ—á.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ:

1. **–ó–∞–ø—Ä–æ—Å —Å–≤–æ–¥–∫–∏**: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç –ø–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
   - –ò—â–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: —Å–≤–æ–¥–∫–∞, —Ä–µ–∑—é–º–µ, –æ–±–∑–æ—Ä, –æ—Ç—á–µ—Ç, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ, —á—Ç–æ –æ–±—Å—É–∂–¥–∞–ª–∏, –∏—Ç–æ–≥–∏
   - –ò–∑–≤–ª–µ–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥ –µ—Å–ª–∏ —É–ø–æ–º—è–Ω—É—Ç: –≤—á–µ—Ä–∞, —Å–µ–≥–æ–¥–Ω—è, –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å, –ø–æ—Å–ª–µ–¥–Ω—è—è –Ω–µ–¥–µ–ª—è, –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü –∏ —Ç.–¥.
   - –°–æ–ø–æ—Å—Ç–∞–≤—å –ø–µ—Ä–∏–æ–¥—ã: "–≤—á–µ—Ä–∞" -> "1d", "–ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å" -> "1h", "–ø–æ—Å–ª–µ–¥–Ω—è—è –Ω–µ–¥–µ–ª—è" -> "1w", "–ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü" -> "1m"
   - –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–π "1h" –µ—Å–ª–∏ –ø–µ—Ä–∏–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω

2. **–û–±—â–µ–Ω–∏–µ/–í–æ–ø—Ä–æ—Å**: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç –ø–æ–æ–±—â–∞—Ç—å—Å—è –∏–ª–∏ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å
   - –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è, –≤–æ–ø—Ä–æ—Å—ã –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –±–æ—Ç–∞, –æ–±—â–µ–Ω–∏–µ
   - –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –ø–æ–ª–µ–∑–Ω—ã–π, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –æ—Ç–≤–µ—Ç –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï

–¢—ã –î–û–õ–ñ–ï–ù –æ—Ç–≤–µ—Ç–∏—Ç—å –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON –æ–±—ä–µ–∫—Ç–æ–º (–±–µ–∑ markdown, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞):

–î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–≤–æ–¥–∫–∏:
{"action": "summary", "timeframe": "1d"}

–î–ª—è –æ–±—â–µ–Ω–∏—è/–≤–æ–ø—Ä–æ—Å–æ–≤:
{"action": "chat", "reply": "–¢–≤–æ–π –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç –∑–¥–µ—Å—å –ù–ê –†–£–°–°–ö–û–ú"}

–ü—Ä–∏–º–µ—Ä—ã:
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: "–î–∞–π —Å–≤–æ–¥–∫—É –∑–∞ –≤—á–µ—Ä–∞"
–û—Ç–≤–µ—Ç: {"action": "summary", "timeframe": "1d"}

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: "–ß—Ç–æ —Ç—ã —É–º–µ–µ—à—å?"
–û—Ç–≤–µ—Ç: {"action": "chat", "reply": "–Ø –º–æ–≥—É —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–≤–æ–¥–∫–∏ –≤–∞—à–µ–π –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞! –ü—Ä–æ—Å—Ç–æ –ø–æ–ø—Ä–æ—Å–∏—Ç–µ –º–µ–Ω—è –æ —Å–≤–æ–¥–∫–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /summary —Å –ø–µ—Ä–∏–æ–¥–æ–º –≤—Ä–µ–º–µ–Ω–∏: 1h, 1d, 1w, 1m, –∏–ª–∏ all."}

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: "–ü–æ–∫–∞–∂–∏ —á—Ç–æ –±—ã–ª–æ –Ω–∞ –ø—Ä–æ—à–ª–æ–π –Ω–µ–¥–µ–ª–µ"
–û—Ç–≤–µ—Ç: {"action": "summary", "timeframe": "1w"}

–í–°–ï –æ—Ç–≤–µ—Ç—ã –≤ –ø–æ–ª–µ reply –î–û–õ–ñ–ù–´ –±—ã—Ç—å –Ω–∞ –†–£–°–°–ö–û–ú —è–∑—ã–∫–µ.
"""

async def detect_intent(user_text: str) -> dict:
    """
    Detect user intent using Groq.
    
    Args:
        user_text: The user's message text
        
    Returns:
        dict: {"action": "summary", "timeframe": "1d"} or
              {"action": "chat", "reply": "response text"}
    """
    try:
        response = await client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": INTENT_SYSTEM_PROMPT},
                {"role": "user", "content": user_text}
            ],
            temperature=0.3,  # Lower temperature for more consistent JSON output
            max_tokens=500,
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # Try to parse JSON
        import json
        try:
            intent_data = json.loads(result_text)
            
            # Validate the response structure
            if "action" not in intent_data:
                raise ValueError("Missing 'action' field in response")
                
            if intent_data["action"] == "summary":
                # Ensure timeframe exists and is valid
                if "timeframe" not in intent_data:
                    intent_data["timeframe"] = "1h"  # Default
                elif intent_data["timeframe"] not in ["1h", "1d", "1w", "1m", "all"]:
                    intent_data["timeframe"] = "1h"  # Fallback to default
                    
            elif intent_data["action"] == "chat":
                # Ensure reply exists
                if "reply" not in intent_data or not intent_data["reply"]:
                    intent_data["reply"] = "–Ø –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å! –í—ã –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ—Å–∏—Ç—å –º–µ–Ω—è –æ —Å–≤–æ–¥–∫–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞."
            else:
                # Unknown action, default to chat
                intent_data = {
                    "action": "chat",
                    "reply": "–ù–µ —É–≤–µ—Ä–µ–Ω, —á—Ç–æ –≤—ã –ø—Ä–æ—Å–∏—Ç–µ. –í—ã –º–æ–∂–µ—Ç–µ –∑–∞–ø—Ä–æ—Å–∏—Ç—å —Å–≤–æ–¥–∫—É –∏–ª–∏ –∑–∞–¥–∞—Ç—å –º–Ω–µ –≤–æ–ø—Ä–æ—Å—ã!"
                }
                
            return intent_data
            
        except json.JSONDecodeError as e:
            logging.error(f"Failed to parse JSON from LLM: {result_text}")
            # Fallback to chat with generic response
            return {
                "action": "chat",
                "reply": "–£ –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º. –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å /summary –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏ —á–∞—Ç–∞!"
            }
            
    except Exception as e:
        logging.error(f"Error in detect_intent: {str(e)}")
        # Fallback response
        return {
            "action": "chat",
            "reply": "–ò–∑–≤–∏–Ω–∏—Ç–µ, —É –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ AI —Å–µ—Ä–≤–∏—Å–∞–º. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /summary."
        }
